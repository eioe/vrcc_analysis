---
title: "VRCC Analysis Pipeline"
author: "AM, FK, PM"
date: "May 5, 2019"
# contact: "klotzsche@cbs.mpg.de", "aleksander.molak@gmail.com", "pawel.motyka@psych.uw.edu.pl"
output: html_document
chunk_output_type: console
self_contained: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
basic_color <- '#195e8c'
```

# Import libraries & setup directories

```{r message=FALSE, warning=FALSE}

# Utils
library(dplyr) 
library(here)
library(haven)
library(readr)
library(tibble)
library(purrr)
library(mosaic)
library(stringr)
library(dplR)

# Plotting and tables
library(ggplot2)
library(sjPlot)

# Statistical tools
library(lme4)
library(lmtest)
library(lmerTest)
library(car)
library(BayesFactor)
library(effsize)
library(psych)

# Load helper functions
source(here("./Code/Analyses/VRTask/Utils/get_cardio_info.R"))
source(here("./Code/Analyses/VRTask/Utils/read_utils.r"))
source(here("./Code/Analyses/VRTask/Utils/distance_utils.r"))


```

<br>

# Build the fulldataset

```{r warning=FALSE}

# Set up the fulldata directory
VRCC_dir         <- here()
data_dir         <- here("Data/VRTask/Logfiles/ExpSubjects")
qstnr_data_path  <- here("./Data/VRTask/Ratings/Ratings_cleaned_AM_2020_10_20.xlsx")
sess_info_folder <- here('./Data/VRTask/Logfiles/ExpSubjects')

# Build the fulldataset (Behavioral/VR data)
cat('\n\nReading bahavioral and VR data...\n\n')
fulldata <- build_dataset(data_dir)

# Get the summary of questionnaire data
cat('\n\nReading questionnaire data...\n\n')
q_data <- get_questnr_data(qstnr_data_path, sess_info_folder)

### Removing the subjects with no cardiac data (temporary - to be decided) 
# # Drop unused data from five subjects
pre_exclusions <- c("S11", "S13", "S18", "S21", "S44")

# from behavioral data:
length(unique(fulldata$ID)) # sample size before
fulldata <- fulldata[!(fulldata$ID %in% pre_exclusions), ]
length(unique(fulldata$ID))  # sample size after

# from questionnaire data:
length(unique(q_data$subject)) # sample size before
q_data <- q_data[!(q_data$subject %in% pre_exclusions), ]
length(unique(q_data$subject))

# Drop unused levels
fulldata <- droplevels(fulldata)

```

<br>

# Compute distance errors

```{r}

# Compute distance errors & add to the fulldataset
fulldata <- fulldata %>% 
  mutate(DistanceError = (EstimatedDistance - RealDistance),
         DistanceErrorAbs = abs(DistanceError),
         DistanceErrorNorm = (DistanceError / RealDistance) * 100,
         DistanceErrorNormAbs = abs(DistanceErrorNorm),
         LocalizationError = compute_euclidean_distances(EstimatedPosition, TruePosition),
         LocalizationErrorNorm = (LocalizationError / RealDistance) * 100,
         AngularErrorRad = compute_angles(EstimatedPosition, TruePosition)) %>% 
  mutate(totTrial = Trial + (Round - 1) * max(Trial)) # Labels the trials with sequential numbers over the whole experiment (1-720)   

# insert NAs when no estimation was made
fulldata$LocalizationError[is.na(fulldata$EstimatedDistance)] <- NA
fulldata$LocalizationErrorNorm[is.na(fulldata$EstimatedDistance)] <- NA
fulldata$AngularErrorRad[is.na(fulldata$EstimatedDistance)] <- NA


```

<br>

* ### Add cardio information to the fulldataset

```{r warning=FALSE, message = FALSE}

# get the list of subjects with full ecg data
subjects_list <- list.files(here("Data/VRTask/Cardio/ExpSubjects/02_Peaks/Events"))
subjects_list <- str_remove(subjects_list, "VRCC_")
subjects_list <- str_remove(subjects_list, ".csv")

cardio_data   <- NULL

# add cardio data
for (subj_ID in subjects_list) {                               
  cardio_temp <- get_cardio_info(fulldata, subj_ID)
  cardio_data <- bind_rows(cardio_data, cardio_temp)
}

# Join the data
fulldata <- full_join(fulldata, cardio_data)                 

# Save the preprocessed data
#write.csv(fulldata, paste0(VRCC_dir, "/Data/VRTask/VRCC_data_unfiltered"), row.names = F)

# Read the preprocessed data
#fulldata <- read.csv(file = paste0(VRCC_dir,"/Data/VRTask/VRCC_data_unfiltered"))


```


<br>

## Exclusions based on localization errors (behavioral & trial-based)


```{r}

# Add unique row ID
fulldata$row_ID <- paste0(fulldata$ID, '_', fulldata$Round, '_', fulldata$Trial)
fulldata$ID <- as.factor(fulldata$ID)

# remove trials with no responses (NAs for distance estimations)
data_filt_behav <- fulldata[!is.na(fulldata$EstimatedDistance),]
nrow(fulldata)-nrow(data_filt_behav) # how many trials
length(unique(fulldata$ID[is.na(fulldata$EstimatedDistance)])) # from how many participants

# remove trials with no cardiac data (either due to temporary absence of ecg signal: 88 trials or too noisy signal: 24 trials; 112 in total)
data_filt_ecg <- data_filt_behav[!is.na(data_filt_behav$systolength),]
nrow(data_filt_behav)-nrow(data_filt_ecg)

# save consolidated data after removal of NAs
data <- data_filt_ecg
rm(data_filt_behav)
rm(data_filt_ecg)

##### TRIAL-BASED ##### 

# Log Localization Error (due to its non-normal distriubion)
data$LocalizationErrorLog <- log(data$LocalizationError)
#hist(data$LocalizationError, breaks = 100)
#hist(data$LocalizationErrorLog, breaks = 100)

# Remove trials with untypically large Localization Errors within subject (LE): LE > avg(LE) + 3*sd(LE)
thrshlds_b <- data %>% 
            group_by(ID) %>% 
            summarize(thrsh_hi = mean(LocalizationErrorLog, na.rm = TRUE) + 3 * sd(LocalizationErrorLog, na.rm = TRUE)) 


fullfltrd_1b <- inner_join(data, thrshlds_b, by = 'ID') %>% 
               filter(LocalizationErrorLog <= thrsh_hi) %>% 
               select(-c(thrsh_hi))


# How many trials were excluded in total?
cat('How many trials were excluded in total?\n')
cat(dim(data)[1] - dim(fullfltrd_1b)[1])

# What percent of trials should be removed at the subject level?
incl_b      <- data    %>% group_by(ID) %>% count()
excl_trial_b <- fullfltrd_1b %>% group_by(ID) %>% count()
comparison_b <- data.frame(incl_b, excl_trial_b)
comparison_b <- comparison_b %>% select(ID, n, n.1)
colnames(comparison_b) <- c('ID', 'INCL', 'EXCL')

# What percent of trials were removed per subject?
comparison_b$diff_trials <- comparison_b$INCL - comparison_b$EXCL
comparison_b$percent_rm  <- (comparison_b$diff_trials / comparison_b$INCL) * 100
describe(comparison_b$percent_rm) # outliers in %
describe(comparison_b$diff_trials) # outlier in absolute numbers

# assign a variable specyfing whether LE value is outlier (alternative/double check)
for (s in unique(data$ID)){
for (t in unique(data$totTrial[data$ID == s])) {
  
ifelse(data$LocalizationErrorLog[data$ID == s & data$totTrial == t] > thrshlds_b$thrsh_hi[thrshlds_b$ID==s], outlier <- "outlier", outlier <- "non_outlier")   
data$LE_outlier[data$ID == s & data$totTrial == t] <- outlier 
}
}

# double check - how many outliers?
nrow(data[data$LE_outlier == "outlier",])

# Exlploratory plot (behavioral data) (export 5 x 5 cm)
group_col <- as.character(data$LE_outlier)            
group_col[group_col == "outlier"] <- scales::alpha("darkorange", 0.5)
group_col[group_col == "non_outlier"] <- scales::alpha("gray20", 0.2)

scatter.smooth(data$RealDistance, data$EstimatedDistance, xlim = c(1.4,5.6), ylim = c(0.5,11.5), cex = 0.2, col = group_col, xlab = "Real Distance", ylab = "Estimated Distance")

# create an identity line using a customized linear model
x<-0:7
y<-0:7
new <- data.frame(x = seq(0, 6, 0.1))
lines(new$x, predict(lm(y~x), new),col= scales::alpha("dodgerblue", alpha = 0.7),lty= 2, lwd = 1.6)

# plot data without outliers
d <- data[data$LE_outlier == "non_outlier",]
scatter.smooth(d$RealDistance, d$EstimatedDistance, xlim = c(1.4,5.6), ylim = c(0.5,8.5), cex = 0.05, col = scales::alpha("gray20", 0.4) , xlab = "Real Distance", ylab = "Estimated Distance")

# create an identity line using a customized linear model
x<-0:7
y<-0:7
new <- data.frame(x = seq(0, 6, 0.1))
lines(new$x, predict(lm(y~x), new),col= scales::alpha("dodgerblue", alpha = 0.8),lty= 2, lwd = 1.6)

```

## Exclusions based on distance estimates (behavioral & subject-based)

```{r}

##### SUBJECT-BASED #####

# Remove subjects with more than three standard deviations above or below the sample mean of Distance Error

# Get subject means
ID_means_b <- fullfltrd_1b %>% group_by(ID) %>% 
                            summarize(mean_DE = mean(DistanceError, na.rm = TRUE))

# Get sample mean & SD
grand_mean_DistanceError <- mean(ID_means_b$mean_DE)
grand_sd_DistanceError   <- sd(ID_means_b$mean_DE)

# Filter-out subjects
fltrd_IDs_b <- ID_means_b %>% filter(mean_DE < grand_mean_DistanceError + 3 * grand_sd_DistanceError) %>% 
                          filter(mean_DE > grand_mean_DistanceError - 3 * grand_sd_DistanceError) 

# Join frames on ID (filter the full data)
fullfltrd_2b <- inner_join(fltrd_IDs_b, fullfltrd_1b, by = 'ID') %>%
               select(-mean_DE)

# mean DE 
hist(fltrd_IDs_b$mean_DE, breaks = 20)

# How many participants were excluded?
cat('How many participants were excluded?\n')
cat(dim(fullfltrd_1b %>% group_by(ID) %>% count())[1] - dim(fullfltrd_2b %>% group_by(ID) %>% count())[1])

# Which ones?
cat('Which ones?\n')
setdiff(ID_means_b, fltrd_IDs_b)

```


## Exclusions based on t_wave ends (for binary analysis)


```{r}
##### TRIAL-BASED #####

# Remove trials with untypically large/short systole lengths
thrshlds_t <- data %>% 
            group_by(ID) %>% 
            summarize(thrsh_hi = mean(systolength, na.rm = TRUE) + 3 * sd(systolength, na.rm = TRUE),                        thrsh_lo = mean(systolength, na.rm = TRUE) - 3 * sd(systolength, na.rm = TRUE)) 

# Remove rows with NA in cardiac data
data_filt <- data %>%  filter(!(is.na(systolength)))

fullfltrd_1t <- inner_join(data_filt, thrshlds_t, by = 'ID') %>% 
               filter(systolength < thrsh_hi) %>% 
               filter(systolength > thrsh_lo) %>%
               select(-c(thrsh_lo, thrsh_hi))


# How many trials were excluded in total?
cat('How many trials were excluded in total?\n')
cat(dim(data_filt)[1] - dim(fullfltrd_1t)[1])

# What percent of trials should be removed at the subject level?
incl_t       <- data_filt %>% group_by(ID) %>% count()
excl_trial_t <- fullfltrd_1t %>% group_by(ID) %>% count()
comparison_t <- data.frame(incl_t, excl_trial_t)
comparison_t <- comparison_t %>% select(ID, n, n.1)
colnames(comparison_t) <- c('ID', 'INCL', 'EXCL')

# What percent of trials were removed per subject?
comparison_t$diff_trials <- comparison_t$INCL - comparison_t$EXCL
comparison_t$percent_rm  <- (comparison_t$diff_trials / comparison_t$INCL) * 100
describe(comparison_t$diff_trials)
describe(comparison_t$percent_rm)

# assign a variable specyfing whether LE value is outlier (alternative/double check)
for (s in unique(data$ID)){
for (t in unique(data$totTrial[data$ID == s])) {
  
ifelse(data$systolength[data$ID == s & data$totTrial == t] > thrshlds_t$thrsh_hi[thrshlds_t$ID==s], outlier <- "outlier", outlier <- "non_outlier") 

ifelse(data$systolength[data$ID == s & data$totTrial == t] < thrshlds_t$thrsh_lo[thrshlds_t$ID==s], outlier <- "outlier", outlier <- outlier)    
data$SYS_outlier[data$ID == s & data$totTrial == t] <- outlier 
}
}

# double check - how many outliers?
nrow(data[data$SYS_outlier == "outlier",])

# Exlploratory plot (behavioral data) (export 5 x 5 cm)
group_col <- as.character(data$SYS_outlier)            
group_col[group_col == "outlier"] <- scales::alpha("darkorange", 0.7)
group_col[group_col == "non_outlier"] <- scales::alpha("gray20", 0.2)

scatter.smooth(data$RRLength, data$systolength, xlim = c(min(data$RRLength),max(data$RRLength)), ylim = c(min(data$systolength),max(data$systolength)), cex = 0.2, col = group_col, xlab = "RR interval", ylab = "Systole length")

# scatter.smooth(data$RRLength[data$ID == "S33"], data$systolength[data$ID == "S33"], xlim = c(min(data$RRLength),max(data$RRLength)), ylim = c(min(data$systolength),max(data$systolength)), cex = 0.2, col = group_col, xlab = "RR interval", ylab = "Systole length")

nrow(data[data$SYS_outlier == "outlier",])
d <- data[data$SYS_outlier == "non_outlier",]

scatter.smooth(d$RRLength, d$systolength, xlim = c(min(d$RRLength),max(d$RRLength)), ylim = c(min(d$systolength),max(d$systolength)), cex = 0.1, col = scales::alpha("gray20", 0.3), xlab = "RR interval", ylab = "Systole length")

```

## Remove individual trials (outliers)

```{r}

data <- data[data$LE_outlier == "non_outlier",]
data <- data[data$SYS_outlier == "non_outlier",]

# identify irregular R peaks (physiologically implausible length of the RR interval; e.g., due to recording errors, motion artefacts, or ventricular extrasystoles)
range(data$RRLength)
hist(data$RRLength, breaks = 100)

# Check whether any of participants had more than 30% of trials excluded based on trial-level criteria or due to data loss (e.g., after early termination of the experiment).
trials_per_subject <- data %>% group_by(ID) %>% count()
trials_per_subject$percent <- (trials_per_subject$n/720) * 100

exclusion_list <- unique(trials_per_subject$ID[trials_per_subject$percent < 70])
data <- data[!(data$ID %in% exclusion_list), ]
range(trials_per_subject$percent) # all subjects had more than 70 % of trials

# drop non-essential columns 
data <- select(data, -Training, -Phase, -LE_outlier, -SYS_outlier)
#write.csv(data, paste0(VRCC_dir, "/Data/VRTask/VRCC_data_consolidated"), row.names = F)


```
