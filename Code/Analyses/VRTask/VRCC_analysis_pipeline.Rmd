---
title: "VRCC Analysis Pipeline"
author: "AM, FK, PM"
date: "May 5, 2019"
# contact: "klotzsche@cbs.mpg.de", "aleksander.molak@gmail.com"
output: html_document
chunk_output_type: console
self_contained: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
basic_color <- '#195e8c'
```

# Import libraries & setup directories

```{r message=FALSE, warning=FALSE}

# Utils
library(dplyr)
library(here)
library(haven)
library(readr)
library(tibble)
library(purrr)

# Plotting and tables
library(ggplot2)
library(sjPlot)

# Statistical tools
library(lme4)
library(lmtest)
library(lmerTest)
library(car)
library(BayesFactor)

# Load helper functions
source(here("./Code/Analyses/VRTask/Utils/get_cardio_info.R"))
source(here("./Code/Analyses/VRTask/Utils/read_utils.r"))

```

<br>

# Build the dataset

```{r warning=FALSE}

# Set up the data directory
VRCC_dir <- here()
data_dir <- here("Data/VRTask/Logfiles/ExpSubjects")

# Build the dataset
fulldata <- build_dataset(data_dir)

```

<br>

# Apply further transformations and create additional variables

```{r}

# Drop unused levels
fulldata <- droplevels(fulldata)

```

<br>

* ### Compute distance errors

```{r}

# Compute distance errors & add to the dataset
fulldata <- fulldata %>% 
  mutate(DistanceError = (EstimatedDistance - RealDistance),
         DistanceErrorAbs = abs(DistanceError),
         DistanceErrorNorm = DistanceError / RealDistance,
         DistanceErrorNormAbs = abs(DistanceErrorNorm)) %>% 
  mutate(totTrial = Trial + (Round - 1) * max(Trial))          # **AM 05-05-19**: What's that?
                                                               # **FK 07-07-19**: Labels the trials with sequential numbers over the whole experiment (1-720)   


```

<br>

* ### Add cardio information to the dataset

```{r warning=FALSE}

subjects_list <- fulldata %>%
                  select(ID) %>%
                  distinct()

cardio_data   <- NULL

for (subject in subjects_list$ID) {                               
  cardio_temp <- get_cardio_info(fulldata, subject)
  cardio_data <- bind_rows(cardio_data, cardio_temp)
}

# Join the data
fulldata <- full_join(fulldata, cardio_data)                 

```

<br>

# Main analysis

<br>

* ### Check if the distance error ($DE$) differs between threatning and non-threatning trials

```{r message=FALSE}

# Check the distribution of DE
ggplot(fulldata, aes(x = DistanceError, fill = FearObject)) + 
                           geom_histogram(alpha = .7) + 
                           xlab('Distance Error') + 
                           ylab('Value') + 
                           theme_minimal()

# Check for normality
qqPlot(fulldata$DistanceError)
shapiro.test(fulldata$DistanceError)

```

<br>

## Testing the main hypothesis (1)

* ### Using two-tailed paired t-test and / or Wilcoxon paired test.

<br>

```{r}
# Split the data
fear_true <- fulldata %>% 
              filter(FearObject == 'True') %>%
              select(DistanceError) %>% pull

fear_flse <- fulldata %>% 
              filter(FearObject == 'False') %>%
              select(DistanceError) %>% pull

# Perform paired t-test
t.test(fear_true, fear_flse, paired = T)
# **FK 23-07-19**: I think it's not ok to (t-)test over all data points here. (assuming independence)
#                  Once we have all data set, we should summarize this by ID/subject.
#                  I'm fairly sure for the standard t-test, not for wilcox and the Bayesian t-test. 
#                  Will you look this up, Aleksander/Pawel? Esp w/ Bayes I have no experience.
# **AM 27-07-19**: Ah, thank you for the comment. Maybe I misunderstood something about the data structure. Could you #                  give me a bit more details on how would you approach this? Do you think we should perform a
#                  serie(s) of t-test for each individual? Or maybe take mean scores per individual and compare
#                  these? As soon as the structure for t-test will be clear we can transfer this approach to
#                  non-param. and bayesian methods.


# Perfom Wilcoxon signed-rank test
wilcox.test(fear_true, fear_flse, paired = T)

```

<br>

* ### Using bayesian t-test (BF).

<br>

```{r}

ttestBF(fear_true, fear_flse, paired = T)

```

<br>

* ### Hypothesis 1: Multilevel model - null model

<br>

```{r}
# Null model
null_model_1 <- lmer(DistanceError ~ (1|ID), 
                     data = fulldata)
# **FK 23-07-19**: In the prereg we put forward a slightly different null model (p. 8) for testing
#                  the 2nd hypothesis:
#                  DistErr ~ 1 + (1 + DistTrue | ID)
#                  Wouldn't it be coherent to use the same one here as well? Opinions welcome.
# **AM 27-07-19**: In my understanding adding (1 + DistTrue ....) would mean that we add random slopes of true 
#                  distance grouped by ID to the model. I'm not sure if I understand the rationle behind this at the 
#                  moment. What would that mean to us?

summary_1_1 <- summary(null_model_1)

```

<br>

* #### Compute ICC of a null model to check if multilevel structure is justified

<br>

```{r}
# AM (2019-07-21): This will be moved to utility functions. This function only works properly for null models.

ICC <- function(model) {
  
  variances = data.frame(summary(model)$varcor)
  
  tau_00    = variances %>% 
                filter(var1 == '(Intercept)') %>% 
                select(vcov) %>% 
                .$vcov
  sigma_2   = variances %>% 
                filter(grp == 'Residual') %>% 
                select(vcov) %>% 
                .$vcov
  
  tau_00 / (tau_00 + sigma_2)
}

# AM (2019-07-21): The below code should stay here:

ICC(null_model_1)

```

<br>

* ### Hypothesis 1: Multilevel model - full model

<br>

```{r}
# Null model
full_model_1 <- lmer(DistanceError ~ FearObject + (1 + FearObject|ID), 
                     data = fulldata)
# **FK 23-07-19**: If we change the null model, the rand. effect DistTrue should probably go in here
#                  as well. Also this makes the model comp. with the next model (2a) possible as they
#                  then are nested. (Pls correct me if I'm bullshitting.)
# **AM 27-07-19**: Yes, totally agree. I propose that we discuss the null model and act here accordingly.

summary_1_2 <- summary(full_model_1)

```

<br>

* ### Hypothesis 1: Compare the two models

```{r}

lrtest(null_model_1, full_model_1)

```

<br>

## Testing the main hypothesis (2a) using multilevel modeling and a circular predictor

<br>

```{r}
print('Implement this...')

```

<br>

## Testing the main hypothesis (2b) using mutlilevel modeling and a binary predictor

<br>

```{r}
print('Implement this...')

```

<br>

# Exploratory / additional analyses

```{r}

mean_error_per_fear <- fulldata %>%
                          group_by(FearObject) %>%
                            summarise(MeanDistanceError = mean(DistanceError, na.rm=T),
                                      MeanDistanceErrorNorm = mean(DistanceErrorNorm, na.rm=T),
                                      MeanDistanceErrorAbs = mean(DistanceErrorAbs, na.rm=T))

mean_error_per_animal <- fulldata %>%
                          group_by(Stimulus) %>%
                            summarise(MeanDistanceError = mean(DistanceError, na.rm=T),
                                      MeanDistanceErrorNorm = mean(DistanceErrorNorm, na.rm=T),
                                      MeanDistanceErrorAbs = mean(DistanceErrorAbs, na.rm=T))
# 
# mean_dist_per_fear
# mean_dist_per_animal
      
```

