---
title: "VRCC Analysis - Testing (early version/temporary)"
author: "AM, FK, PM"
date: "May 5, 2019"
# contact: "klotzsche@cbs.mpg.de", "aleksander.molak@gmail.com", "pawel.motyka@psych.uw.edu.pl"
output: html_document
chunk_output_type: console
self_contained: no
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
basic_color <- '#195e8c'
```

# Import libraries & setup directories

```{r message=FALSE, warning=FALSE}

# Utils
library(dplyr) 
library(here)
library(haven)
library(readr)
library(tibble)
library(purrr)
library(mosaic)
library(stringr)

# Plotting and tables
library(ggplot2)
library(sjPlot)

# Statistical tools
library(lme4)
library(lmtest)
library(lmerTest)
library(car)
library(BayesFactor)
library(effsize)
library(psych)

```

<br>

# Build the fulldataset

```{r warning=FALSE, fig.height = 3, fig.width = 4}

# Set up the fulldata directory
VRCC_dir         <- here()

### Explore the relationship between Distance Error and Real Distance (possibly the side-branch in the final code)

# Read the preprocessed data
data <- read.csv(file = paste0(VRCC_dir,"/Data/VRTask/VRCC_data_consolidated"))

# VRCC_behav_pilots includes other attempts of approaching this issue

# for (p in unique(fulldata$ID)) {
# 
# subdata <- fulldata[fulldata$ID == p,]
#   
# Figure <- ggplot(data = subdata, aes(x = RealDistance, y = DistanceError)) + 
#   geom_point(col = "grey 15", alpha = 0.3, size = 2.5, shape = 18) + 
#   geom_smooth(col = "grey8", method = "loess", level=0.95, alpha = 0.8) + 
#   labs(y = "Distance Error", x = "True Distance") +   
#   ggtitle(p) +
#   theme_classic() + 
#   theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 17), axis.text=element_text(size=15),  axis.title=element_text(size=15)) +
#   scale_x_continuous(limits= c(min(fulldata$RealDistance), max(fulldata$RealDistance)), expand = c(0.01,0.01)) +
#   scale_y_continuous(limits= c(min(fulldata$DistanceError), max(fulldata$DistanceError)), expand = c(0.01,0.01)) 
# 
# plot(Figure)
# 
# }


```

<br>

# Main analysis - Hypothesis 1

<br>

* ### Check if the distance error ($DE$) differs between threatening and non-threatening trials

```{r message=FALSE}

# Check the distribution of DE (all data points - not averaged for individuals)
ggplot(data, aes(x = DistanceError, fill = FearObject)) + 
                           geom_histogram(alpha = .7) + 
                           xlab('Distance Error') + 
                           ylab('Value') + 
                           theme_minimal()
qqPlot(data$DistanceError)

## Testing the hypothesis 1. (T-test)



# Derive individual means of distance error for threatening and non-threatening objects

data_by_id <- data %>%
  group_by(ID) %>%
  summarize(mean_threat     = mean(DistanceError[FearObject == "True"]), # threatening objects
            mean_non_threat = mean(DistanceError[FearObject == "False"])) # non-threatening objects

data_by_id_2 <- data %>%
  group_by(ID) %>%
  summarize(mean_threat_syst      = mean(DistanceError[FearObject == "True" & c_phase == 'systole']), 
            mean_non_threat_syst  = mean(DistanceError[FearObject == "False" & c_phase == 'systole']),
            mean_threat_diast     = mean(DistanceError[FearObject == "True" & c_phase == 'diastole']), 
            mean_non_threat_diast = mean(DistanceError[FearObject == "False" & c_phase == 'diastole'])
            ) 

d <- data_by_id_2 
d$diff <- d$mean_threat_syst - d$mean_threat_diast
mean(d$mean_threat_syst)
mean(d$mean_threat_diast)
hist(d$diff)

#shapiro.test(data_ID$mean_threat)
#shapiro.test(data_ID$mean_non_threat)

# Employ paired t-test to determine whether distance error differs between the threatening and non-threatening objects
t.test(data_by_id$mean_threat, data_by_id$mean_non_threat, alt = "two.sided", conf = 0.95, paired = T) 

t.test(data_by_id_2$mean_threat_syst, data_by_id_2$mean_threat_diast, alt = "two.sided", conf = 0.95, paired = T) 
## PM: we could even consider "one.sided" test here
# Perfom Wilcoxon signed-rank test:  
#wilcox.test(data_ID$mean_threat, data_ID$mean_threat, paired = T)

# Show summary statistics for threatening and non-threatening objects:
mean(data_by_id$mean_threat)
sd(data_by_id$mean_threat)

mean(data_by_id$mean_non_threat)
sd(data_by_id$mean_non_threat)

# Check the normality of dependent variable distribution
#shapiro.test(data_by_id$mean_threat)
#shapiro.test(data_by_id$mean_non_threat)

# Employ paired t-test to determine whether distance error differs between the threatening and non-threatening objects
t.test(data_by_id$mean_threat, data_by_id$mean_non_threat, alt = "two.sided", conf = 0.95, paired = T) ## PM: we could even consider "one.sided" test here
# Perfom Wilcoxon signed-rank test:  
#wilcox.test(data_by_id$mean_threat, data_by_id$mean_threat, paired = T)

# Show summary statistics for threatening and non-threatening objects:
mean(data_by_id$mean_threat)
sd(data_by_id$mean_threat)

mean(data_by_id$mean_non_threat)
sd(data_by_id$mean_non_threat)

# show number of participants with lower distance error values for threatening objects
nrow(data_by_id[data_by_id$mean_threat < data_by_id$mean_non_threat,])

# Calculate Bayes Factor
ttestBF(data_by_id$mean_threat, data_by_id$mean_non_threat, paired = T)

## Testing the hypothesis 1. (Regression) #PM: This is to some extent analogous to further sections (and only a draft for now); We need to discuss our approach towards testing H1 with regression

null_model_H1 <- lmer(DistanceError ~ 1 + (1 + RealDistance | ID), data = data)
summary(null_model_H1)

model_H1 <- lmer(DistanceError ~ FearObject + (1 + RealDistance | ID), data = data)
summary(model_H1)

```

<br>

* ### Hypothesis 1: Multilevel model - null model

### REMEMBER TO EXCLUDE `c_phase == 'buffer'` FROM BINARY ANALYSIS ####

<br>

```{r}
# Null model
null_model_1 <- lmer(DistanceError ~ (1|ID), 
                     data = data)
# **FK 23-07-19**: In the prereg we put forward a slightly different null model (p. 8) for testing
#                  the 2nd hypothesis:
#                  DistErr ~ 1 + (1 + DistTrue | ID)
#                  Wouldn't it be coherent to use the same one here as well? Opinions welcome.
# **AM 27-07-19**: In my understanding adding (1 + DistTrue ....) would mean that we add random slopes of true 
#                  distance grouped by ID to the model. I'm not sure if I understand the rationle behind this at the 
#                  moment. What would that mean to us?

summary_1_1 <- summary(null_model_1)
summary_1_1 

```

<br>

* #### Compute ICC of a null model to check if multilevel structure is justified

<br>

```{r}
# AM (2019-07-21): This will be moved to utility functions. This function only works properly for null models.

ICC <- function(model) {
  
  variances = data.frame(summary(model)$varcor)
  
  tau_00    = variances %>% 
                filter(var1 == '(Intercept)') %>% 
                select(vcov) %>% 
                .$vcov
  sigma_2   = variances %>% 
                filter(grp == 'Residual') %>% 
                select(vcov) %>% 
                .$vcov
  
  tau_00 / (tau_00 + sigma_2)
}

# AM (2019-07-21): The below code should stay here:

ICC(null_model_1)

```

<br>

* ### Hypothesis 1: Multilevel model - full model

<br>

```{r}
# Null model
full_model_1 <- lmer(DistanceError ~ FearObject + (1 + FearObject|ID), 
                     data = data)
# **FK 23-07-19**: If we change the null model, the rand. effect DistTrue should probably go in here
#                  as well. Also this makes the model comp. with the next model (2a) possible as they
#                  then are nested. (Pls correct me if I'm bullshitting.)
# **AM 27-07-19**: Yes, totally agree. I propose that we discuss the null model and act here accordingly.

summary_1_2 <- summary(full_model_1)

```

<br>

* ### Hypothesis 1: Compare the two models

```{r}

lrtest(null_model_1, full_model_1)

```

<br>

## Testing the main hypothesis (2a) using multilevel modeling and a circular predictor

<br>

```{r}
print('Implement this...')

```

<br>

## Testing the main hypothesis (2b) using mutlilevel modeling and a binary predictor

<br>

```{r}
print('Implement this...')

```

<br>

# Exploratory / additional analyses

```{r}

# Needs aggregation of depedent variables for each ID

mean_error_per_fear <- data %>%
                          group_by(FearObject) %>%
                            summarise(MeanDistanceError = mean(DistanceError, na.rm=T),
                                      MeanDistanceErrorNorm = mean(DistanceErrorNorm, na.rm=T),
                                      MeanDistanceErrorAbs = mean(DistanceErrorAbs, na.rm=T))

mean_error_per_animal <- data %>%
                          group_by(Stimulus) %>%
                            summarise(MeanDistanceError = mean(DistanceError, na.rm=T),
                                      MeanDistanceErrorNorm = mean(DistanceErrorNorm, na.rm=T),
                                      MeanDistanceErrorAbs = mean(DistanceErrorAbs, na.rm=T))
# 
# mean_dist_per_fear
# mean_dist_per_animal
      
```

